{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random, os\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.5\n",
    "min_df = 5  # choose desired value for min_df\n",
    "\n",
    "# Read stopwords\n",
    "with open('stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"../raw_data/amazon_home_20000_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns={'text': 'data'}).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=77, stratify=dataset.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16158</td>\n",
       "      <td>item grow fresh herb year last long waste mone...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16457</td>\n",
       "      <td>concern product review report problem dirty wa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8497</td>\n",
       "      <td>good size sturdy material bend several time dr...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>roast chicken first time work second time thic...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16871</td>\n",
       "      <td>small powerful hold standard piece paper fridg...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9132</td>\n",
       "      <td>awesome turn sideways leak lunch upright waste...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19197</td>\n",
       "      <td>cook twine cheap throw dirty somehow nice cont...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15262</td>\n",
       "      <td>rack wood clothe coat plastic catch strongest ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15896</td>\n",
       "      <td>cute sticker look great picture option thank</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1828</td>\n",
       "      <td>tire capsule coffeemaker superexpensive capsul...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data  sentiment\n",
       "16158  item grow fresh herb year last long waste mone...        5.0\n",
       "16457  concern product review report problem dirty wa...        4.0\n",
       "8497   good size sturdy material bend several time dr...        5.0\n",
       "174    roast chicken first time work second time thic...        1.0\n",
       "16871  small powerful hold standard piece paper fridg...        3.0\n",
       "...                                                  ...        ...\n",
       "9132   awesome turn sideways leak lunch upright waste...        2.0\n",
       "19197  cook twine cheap throw dirty somehow nice cont...        5.0\n",
       "15262  rack wood clothe coat plastic catch strongest ...        4.0\n",
       "15896       cute sticker look great picture option thank        5.0\n",
       "1828   tire capsule coffeemaker superexpensive capsul...        5.0\n",
       "\n",
       "[14000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', doc) for doc in train_data.data]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', doc) for doc in test_data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_docs = init_docs_tr + init_docs_ts\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "init_docs = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n"
     ]
    }
   ],
   "source": [
    "# Create count vectorizer\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "cvz = cvectorizer.fit_transform(init_docs).sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the vocabulary...\n",
      "  initial vocabulary size: 4077\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0)\n",
    "v_size = sum_counts.shape[1]\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n",
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n",
    "del cvectorizer\n",
    "print('  initial vocabulary size: {}'.format(v_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort elements in vocabulary\n",
    "idx_sort = np.argsort(sum_counts_np)\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary size after removing stopwords from list: 3954\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords (if any)\n",
    "vocab_aux = [w for w in vocab_aux if w not in stops]\n",
    "print('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n"
     ]
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs_tr = len(init_docs_tr)\n",
    "trSize = num_docs_tr-100\n",
    "tsSize = len(init_docs_ts)\n",
    "vaSize = 100\n",
    "idx_permute = np.random.permutation(num_docs_tr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary after removing words not in train: 3951\n"
     ]
    }
   ],
   "source": [
    "# Remove words not in train_data\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in init_docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train/test/valid\n",
    "docs_tr = [[word2id[w] for w in init_docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "docs_va = [[word2id[w] for w in init_docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "docs_ts = [[word2id[w] for w in init_docs[idx_d+num_docs_tr].split() if w in word2id] for idx_d in range(tsSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  number of documents (train): 13900 [this should be equal to 13900]\n",
      "  number of documents (test): 6000 [this should be equal to 6000]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n"
     ]
    }
   ],
   "source": [
    "print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize))\n",
    "print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize))\n",
    "print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing empty documents...\n"
     ]
    }
   ],
   "source": [
    "# Remove empty documents\n",
    "print('removing empty documents...')\n",
    "\n",
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "docs_tr = remove_empty(docs_tr)\n",
    "docs_ts = remove_empty(docs_ts)\n",
    "docs_va = remove_empty(docs_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  230350\n",
      "  len(words_ts):  99524\n",
      "  len(words_ts_h1):  48244\n",
      "  len(words_ts_h2):  51280\n",
      "  len(words_va):  1660\n"
     ]
    }
   ],
   "source": [
    "# Remove test documents with length=1\n",
    "docs_ts = [doc for doc in docs_ts if len(doc)>1]\n",
    "\n",
    "# Split test set in 2 halves\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n",
    "\n",
    "# Getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr): ', len(words_tr))\n",
    "print('  len(words_ts): ', len(words_ts))\n",
    "print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('  len(words_va): ', len(words_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 13900 [this should be 13900]\n",
      "  len(np.unique(doc_indices_ts)): 6000 [this should be 6000]\n",
      "  len(np.unique(doc_indices_ts_h1)): 6000 [this should be 6000]\n",
      "  len(np.unique(doc_indices_ts_h2)): 6000 [this should be 6000]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n"
     ]
    }
   ],
   "source": [
    "# Get doc indices\n",
    "print('getting doc indices...')\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "# Remove unused variables\n",
    "del docs_tr\n",
    "del docs_ts\n",
    "del docs_ts_h1\n",
    "del docs_ts_h2\n",
    "del docs_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating bow representation...\n"
     ]
    }
   ],
   "source": [
    "# Create bow representation\n",
    "print('creating bow representation...')\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "del words_tr\n",
    "del words_ts\n",
    "del words_ts_h1\n",
    "del words_ts_h2\n",
    "del words_va\n",
    "del doc_indices_tr\n",
    "del doc_indices_ts\n",
    "del doc_indices_ts_h1\n",
    "del doc_indices_ts_h2\n",
    "del doc_indices_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the vocabulary to a file\n",
    "path_save = '../data/ah20k/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.system('mkdir -p ' + path_save)\n",
    "\n",
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "*************\n"
     ]
    }
   ],
   "source": [
    "# Split bow intro token/value pairs\n",
    "print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts\n",
    "\n",
    "print('Data ready !!')\n",
    "print('*************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
